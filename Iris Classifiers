{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c20ccbd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-10T16:26:17.472736Z",
     "iopub.status.busy": "2024-11-10T16:26:17.472267Z",
     "iopub.status.idle": "2024-11-10T16:26:35.156813Z",
     "shell.execute_reply": "2024-11-10T16:26:35.155754Z"
    },
    "papermill": {
     "duration": 17.693305,
     "end_time": "2024-11-10T16:26:35.159671",
     "exception": false,
     "start_time": "2024-11-10T16:26:17.466366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#i data handling libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore display of unnecessary warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "# data preprocessing libs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn classifiers to import\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# tensorflow classifier import fix it \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# * OLD ONE from tensorflow.contrib.learn import DNNClassifier\n",
    "\n",
    "# model building, predict, accuracy imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf32633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:26:35.168880Z",
     "iopub.status.busy": "2024-11-10T16:26:35.167541Z",
     "iopub.status.idle": "2024-11-10T16:26:35.208614Z",
     "shell.execute_reply": "2024-11-10T16:26:35.207457Z"
    },
    "papermill": {
     "duration": 0.048235,
     "end_time": "2024-11-10T16:26:35.211238",
     "exception": false,
     "start_time": "2024-11-10T16:26:35.163003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: Iris Data set\n",
      "Number of instances in dataset: 151\n",
      "Number of attributes in dataset: 4\n"
     ]
    }
   ],
   "source": [
    "#i\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"/kaggle/input/iriscsv/iris.csv\", names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "print('Dataset used: Iris Data set')\n",
    "print('Number of instances in dataset:', len(data))\n",
    "print('Number of attributes in dataset:', len(data.columns) - 1)\n",
    "num_folds = 15\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "data['species'] = le.fit_transform(data['species'])\n",
    "\n",
    "# Convert strings to numeric and handle any NaNs\n",
    "data['sepal_length'] = pd.to_numeric(data['sepal_length'], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split features and target\n",
    "X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.18, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e40853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:26:35.219456Z",
     "iopub.status.busy": "2024-11-10T16:26:35.218852Z",
     "iopub.status.idle": "2024-11-10T16:38:06.936015Z",
     "shell.execute_reply": "2024-11-10T16:38:06.934650Z"
    },
    "papermill": {
     "duration": 691.724128,
     "end_time": "2024-11-10T16:38:06.938464",
     "exception": false,
     "start_time": "2024-11-10T16:26:35.214336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 fold cross-validation is used\n",
      "\n",
      "Computing GridSearch on RandomForest...\n",
      "Computing GridSearch on DecisionTree...\n",
      "Computing GridSearch on Perceptron...\n",
      "Computing GridSearch on SVM...\n",
      "Computing GridSearch on NeuralNetwork...\n",
      "Computing GridSearch on LogisticRegression...\n",
      "Computing GridSearch on KNearestNeighbors...\n",
      "Computing GridSearch on Bagging...\n",
      "Computing GridSearch on AdaBoost...\n",
      "Computing GridSearch on Naive-Bayes...\n",
      "Computing GridSearch on GradientBoosting...\n",
      "\n",
      "============================================================\n",
      "RandomForest: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 93.4259%\n",
      "DecisionTree: Accuracy with Best Parameters = 96.2963% || Mean Cross Validation Accuracy = 92.5926%\n",
      "Perceptron: Accuracy with Best Parameters = 77.7778% || Mean Cross Validation Accuracy = 78.8889%\n",
      "SVM: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 95.0%\n",
      "NeuralNetwork: Accuracy with Best Parameters = 96.2963% || Mean Cross Validation Accuracy = 97.5%\n",
      "LogisticRegression: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 96.6667%\n",
      "KNearestNeighbors: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 94.2593%\n",
      "Bagging: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 95.0926%\n",
      "AdaBoost: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 93.4259%\n",
      "Naive-Bayes: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 94.2593%\n",
      "GradientBoosting: Accuracy with Best Parameters = 100.0% || Mean Cross Validation Accuracy = 94.2593%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grids\n",
    "random_forest_params = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [2, 3, 4, 'auto', 'log2', 'sqrt', None],\n",
    "    'bootstrap': [False, True]\n",
    "}\n",
    "decision_tree_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'max_features': [2, 3, 'auto', 'log2', 'sqrt', None],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "perceptron_params = {\n",
    "    'penalty': [None, 'l2', 'l1', 'elasticnet'],\n",
    "    'fit_intercept': [False, True],\n",
    "    'shuffle': [False, True],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'alpha': [0.0001, 0.00025],\n",
    "    'max_iter': [30, 50, 90]\n",
    "}\n",
    "svm_params = {\n",
    "    'shrinking': [False, True],\n",
    "    'degree': [3, 4],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "neural_net_params = {\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(20, 15, 10), (30, 20, 15, 10), (16, 8, 4)],\n",
    "    'max_iter': [50, 80, 150],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'shuffle': [True, False]\n",
    "}\n",
    "log_reg_params = {\n",
    "    'class_weight': ['balanced', None],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "knn_params = {\n",
    "    'n_neighbors': [2, 3, 5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [5, 10, 15, 20]\n",
    "}\n",
    "bagging_params = {\n",
    "    'n_estimators': [5, 12, 15, 20],\n",
    "    'bootstrap': [False, True]\n",
    "}\n",
    "ada_boost_params = {\n",
    "    'n_estimators': [50, 75, 100],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "gradient_boosting_params = {\n",
    "    'n_estimators': [15, 25, 50]\n",
    "}\n",
    "\"\"\"\n",
    "# Build parameters of all classifiers\n",
    "random_forest_params = dict(n_estimators=[5, 10, 15, 20, 25], criterion=['gini', 'entropy'], \n",
    "                            max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None], bootstrap=[False, True]\n",
    "                            )\n",
    "decision_tree_params = dict(criterion=['gini', 'entropy'], splitter=['best', 'random'], min_samples_split=[2, 3, 4],\n",
    "                            max_features=[2,3,'auto', 'log2', 'sqrt', None], class_weight=['balanced', None], presort=[False, True])\n",
    "\n",
    "perceptron_params = dict(penalty=[None, 'l2', 'l1', 'elasticnet'], fit_intercept=[False, True], shuffle=[False, True],\n",
    "                         class_weight=['balanced', None], alpha=[0.0001, 0.00025], max_iter=[30,50,90])\n",
    "\n",
    "svm_params = dict(shrinking=[False, True], degree=[3,4], class_weight=['balanced', None])\n",
    "\n",
    "neural_net_params = dict(activation=['identity', 'logistic', 'tanh', 'relu'], hidden_layer_sizes = [(20,15,10),(30,20,15,10),(16,8,4)], \n",
    "                         max_iter=[50,80,150], solver=['adam','lbfgs'], learning_rate=['constant', 'invscaling', 'adaptive'], shuffle=[True, False])\n",
    "\n",
    "log_reg_params = dict(class_weight=['balanced', None], solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'], fit_intercept=[True, False])\n",
    "\n",
    "knn_params = dict(n_neighbors=[2, 3, 5, 10], weights=['uniform', 'distance'],\n",
    "                  algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], leaf_size=[5,10,15,20])\n",
    "\n",
    "bagging_params = dict(n_estimators=[5, 12, 15, 20], bootstrap=[False, True])\n",
    "\n",
    "ada_boost_params = dict(n_estimators=[50, 75, 100], algorithm=['SAMME', 'SAMME.R'])\n",
    "\n",
    "guassiannb_params = dict()\n",
    "\n",
    "gradient_boosting_params = dict(n_estimators=[15, 25, 50])\n",
    "\n",
    "params = [\n",
    "    random_forest_params, decision_tree_params, perceptron_params,\n",
    "    svm_params, neural_net_params, log_reg_params, knn_params,\n",
    "    bagging_params, ada_boost_params, guassiannb_params, gradient_boosting_params\n",
    "]\n",
    "\n",
    "# classifiers to test\n",
    "classifiers = [\n",
    "    RandomForestClassifier(), DecisionTreeClassifier(), Perceptron(),\n",
    "    SVC(), MLPClassifier(), LogisticRegression(),\n",
    "    KNeighborsClassifier(), BaggingClassifier(), AdaBoostClassifier(),\n",
    "    GaussianNB(), GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'RandomForest', 'DecisionTree', 'Perceptron', 'SVM',\n",
    "    'NeuralNetwork', 'LogisticRegression',\n",
    "    'KNearestNeighbors', 'Bagging', 'AdaBoost', 'Naive-Bayes', 'GradientBoosting'\n",
    "]\n",
    "\n",
    "models = dict(zip(names, zip(classifiers, params)))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Models and their parameters\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), random_forest_params),\n",
    "    'DecisionTree': (DecisionTreeClassifier(), decision_tree_params),\n",
    "    'Perceptron': (Perceptron(), perceptron_params),\n",
    "    'SVM': (SVC(), svm_params),\n",
    "    'NeuralNetwork': (MLPClassifier(), neural_net_params),\n",
    "    'LogisticRegression': (LogisticRegression(), log_reg_params),\n",
    "    'KNearestNeighbors': (KNeighborsClassifier(), knn_params),\n",
    "    'Bagging': (BaggingClassifier(), bagging_params),\n",
    "    'AdaBoost': (AdaBoostClassifier(), ada_boost_params),\n",
    "    'Naive-Bayes': (GaussianNB(), {}),\n",
    "    'GradientBoosting': (GradientBoostingClassifier(), gradient_boosting_params)\n",
    "}\n",
    "\n",
    "# Parameter tuning with GridSearchCV\n",
    "def parameter_tuning(models, X_train, X_test, y_train, y_test):\n",
    "    print(num_folds, 'fold cross-validation is used\\n')\n",
    "    accuracies = []\n",
    "    best_parameters = []\n",
    "    for name, (clf, clf_params) in models.items():\n",
    "        print(f'Computing GridSearch on {name}...')\n",
    "        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params, cv=num_folds)\n",
    "        grid_clf.fit(X_train, y_train)\n",
    "        best_parameters.append((name, grid_clf.best_params_))\n",
    "        predictions = grid_clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n",
    "        accuracies.append((name, accuracy, np.mean(cv_scores)))\n",
    "    return accuracies, best_parameters\n",
    "\n",
    "# Run parameter tuning and display results\n",
    "results, best_parameters = parameter_tuning(models, X_train, X_test, y_train, y_test)\n",
    "print('\\n============================================================')\n",
    "for classifier, acc, cv_acc in results:\n",
    "    print(f'{classifier}: Accuracy with Best Parameters = {round(acc * 100, 4)}% || Mean Cross Validation Accuracy = {round(cv_acc * 100, 4)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c46f355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:38:06.948728Z",
     "iopub.status.busy": "2024-11-10T16:38:06.948276Z",
     "iopub.status.idle": "2024-11-10T16:49:36.089114Z",
     "shell.execute_reply": "2024-11-10T16:49:36.087864Z"
    },
    "papermill": {
     "duration": 689.149756,
     "end_time": "2024-11-10T16:49:36.092252",
     "exception": false,
     "start_time": "2024-11-10T16:38:06.942496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 fold cross-validation is used\n",
      "\n",
      "Computing GridSearch on RandomForest...\n",
      "Computing GridSearch on DecisionTree...\n",
      "Computing GridSearch on Perceptron...\n",
      "Computing GridSearch on SVM...\n",
      "Computing GridSearch on NeuralNetwork...\n",
      "Computing GridSearch on LogisticRegression...\n",
      "Computing GridSearch on KNearestNeighbors...\n",
      "Computing GridSearch on Bagging...\n",
      "Computing GridSearch on AdaBoost...\n",
      "Computing GridSearch on Naive-Bayes...\n",
      "Computing GridSearch on GradientBoosting...\n",
      "\n",
      "============================================================\n",
      "RandomForest Accuracy on Test Set: 1.00, Mean CV Score: 0.93\n",
      "DecisionTree Accuracy on Test Set: 1.00, Mean CV Score: 0.93\n",
      "Perceptron Accuracy on Test Set: 0.78, Mean CV Score: 0.79\n",
      "SVM Accuracy on Test Set: 1.00, Mean CV Score: 0.95\n",
      "NeuralNetwork Accuracy on Test Set: 1.00, Mean CV Score: 0.97\n",
      "LogisticRegression Accuracy on Test Set: 1.00, Mean CV Score: 0.97\n",
      "KNearestNeighbors Accuracy on Test Set: 1.00, Mean CV Score: 0.94\n",
      "Bagging Accuracy on Test Set: 1.00, Mean CV Score: 0.93\n",
      "AdaBoost Accuracy on Test Set: 1.00, Mean CV Score: 0.93\n",
      "Naive-Bayes Accuracy on Test Set: 1.00, Mean CV Score: 0.94\n",
      "GradientBoosting Accuracy on Test Set: 1.00, Mean CV Score: 0.93\n",
      "\n",
      "Best Parameters for each model:\n",
      "RandomForest: {'bootstrap': True, 'criterion': 'gini', 'max_features': 3, 'n_estimators': 10}\n",
      "DecisionTree: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_split': 3, 'splitter': 'best'}\n",
      "Perceptron: {'alpha': 0.00025, 'class_weight': 'balanced', 'fit_intercept': True, 'max_iter': 30, 'penalty': 'l1', 'shuffle': True}\n",
      "SVM: {'class_weight': 'balanced', 'degree': 3, 'shrinking': False}\n",
      "NeuralNetwork: {'activation': 'tanh', 'hidden_layer_sizes': (20, 15, 10), 'learning_rate': 'adaptive', 'max_iter': 150, 'shuffle': True, 'solver': 'adam'}\n",
      "LogisticRegression: {'class_weight': 'balanced', 'fit_intercept': True, 'solver': 'sag'}\n",
      "KNearestNeighbors: {'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Bagging: {'bootstrap': True, 'n_estimators': 5}\n",
      "AdaBoost: {'algorithm': 'SAMME.R', 'n_estimators': 100}\n",
      "Naive-Bayes: {}\n",
      "GradientBoosting: {'n_estimators': 15}\n"
     ]
    }
   ],
   "source": [
    "#2.  Hyperparameter grids\n",
    "random_forest_params = dict(\n",
    "    n_estimators=[5, 10, 15, 20, 25],\n",
    "    criterion=['gini', 'entropy'],\n",
    "    max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None],\n",
    "    bootstrap=[False, True]\n",
    ")\n",
    "decision_tree_params = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    splitter=['best', 'random'],\n",
    "    min_samples_split=[2, 3, 4],\n",
    "    max_features=[2, 3, 'auto', 'log2', 'sqrt', None],\n",
    "    class_weight=['balanced', None]\n",
    ")\n",
    "perceptron_params = dict(\n",
    "    penalty=[None, 'l2', 'l1', 'elasticnet'],\n",
    "    fit_intercept=[False, True],\n",
    "    shuffle=[False, True],\n",
    "    class_weight=['balanced', None],\n",
    "    alpha=[0.0001, 0.00025],\n",
    "    max_iter=[30, 50, 90]\n",
    ")\n",
    "svm_params = dict(\n",
    "    shrinking=[False, True],\n",
    "    degree=[3, 4],\n",
    "    class_weight=['balanced', None]\n",
    ")\n",
    "neural_net_params = dict(\n",
    "    activation=['identity', 'logistic', 'tanh', 'relu'],\n",
    "    hidden_layer_sizes=[(20, 15, 10), (30, 20, 15, 10), (16, 8, 4)],\n",
    "    max_iter=[50, 80, 150],\n",
    "    solver=['adam', 'lbfgs'],\n",
    "    learning_rate=['constant', 'invscaling', 'adaptive'],\n",
    "    shuffle=[True, False]\n",
    ")\n",
    "log_reg_params = dict(\n",
    "    class_weight=['balanced', None],\n",
    "    solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "    fit_intercept=[True, False]\n",
    ")\n",
    "knn_params = dict(\n",
    "    n_neighbors=[2, 3, 5, 10],\n",
    "    weights=['uniform', 'distance'],\n",
    "    algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    leaf_size=[5, 10, 15, 20]\n",
    ")\n",
    "bagging_params = dict(\n",
    "    n_estimators=[5, 12, 15, 20],\n",
    "    bootstrap=[False, True]\n",
    ")\n",
    "ada_boost_params = dict(\n",
    "    n_estimators=[50, 75, 100],\n",
    "    algorithm=['SAMME', 'SAMME.R']\n",
    ")\n",
    "gradient_boosting_params = dict(\n",
    "    n_estimators=[15, 25, 50]\n",
    ")\n",
    "\n",
    "# Models and their parameters\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), random_forest_params),\n",
    "    'DecisionTree': (DecisionTreeClassifier(), decision_tree_params),\n",
    "    'Perceptron': (Perceptron(), perceptron_params),\n",
    "    'SVM': (SVC(), svm_params),\n",
    "    'NeuralNetwork': (MLPClassifier(), neural_net_params),\n",
    "    'LogisticRegression': (LogisticRegression(), log_reg_params),\n",
    "    'KNearestNeighbors': (KNeighborsClassifier(), knn_params),\n",
    "    'Bagging': (BaggingClassifier(), bagging_params),\n",
    "    'AdaBoost': (AdaBoostClassifier(), ada_boost_params),\n",
    "    'Naive-Bayes': (GaussianNB(), {}),\n",
    "    'GradientBoosting': (GradientBoostingClassifier(), gradient_boosting_params)\n",
    "}\n",
    "\n",
    "# Parameter tuning with GridSearchCV\n",
    "def parameter_tuning(models, X_train, X_test, y_train, y_test):\n",
    "    print(num_folds, 'fold cross-validation is used\\n')\n",
    "    accuracies = []\n",
    "    best_parameters = []\n",
    "    for name, (clf, clf_params) in models.items():\n",
    "        print(f'Computing GridSearch on {name}...')\n",
    "        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params, cv=num_folds)\n",
    "        grid_clf.fit(X_train, y_train)\n",
    "        best_parameters.append((name, grid_clf.best_params_))\n",
    "        predictions = grid_clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n",
    "        accuracies.append((name, accuracy, np.mean(cv_scores)))\n",
    "    return accuracies, best_parameters\n",
    "\n",
    "# Run parameter tuning and display results\n",
    "results, best_parameters = parameter_tuning(models, X_train, X_test, y_train, y_test)\n",
    "print('\\n============================================================')\n",
    "for classifier_name, accuracy, cv_mean in results:\n",
    "    print(f\"{classifier_name} Accuracy on Test Set: {accuracy:.2f}, Mean CV Score: {cv_mean:.2f}\")\n",
    "\n",
    "print('\\nBest Parameters for each model:')\n",
    "for name, params in best_parameters:\n",
    "    print(f\"{name}: {params}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1836703,
     "sourceId": 2997853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1404.144167,
   "end_time": "2024-11-10T16:49:38.551679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-10T16:26:14.407512",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
