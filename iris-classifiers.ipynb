{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2997853,"sourceType":"datasetVersion","datasetId":1836703}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#i data handling libs\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n# Ignore display of unnecessary warnings\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\nwarnings.filterwarnings(\"ignore\")\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\n    \n# data preprocessing libs\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# sklearn classifiers to import\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# tensorflow classifier import fix it \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n# * OLD ONE from tensorflow.contrib.learn import DNNClassifier\n\n# model building, predict, accuracy imports\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom IPython.display import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-10T16:20:15.324766Z","iopub.execute_input":"2024-11-10T16:20:15.325195Z","iopub.status.idle":"2024-11-10T16:20:30.526274Z","shell.execute_reply.started":"2024-11-10T16:20:15.325156Z","shell.execute_reply":"2024-11-10T16:20:30.525155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#i\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")\ntf.get_logger().setLevel('FATAL')\n\n# Load dataset\ndata = pd.read_csv(\"/kaggle/input/iriscsv/iris.csv\", names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\nprint('Dataset used: Iris Data set')\nprint('Number of instances in dataset:', len(data))\nprint('Number of attributes in dataset:', len(data.columns) - 1)\nnum_folds = 15\n\n# Encode target labels\nle = LabelEncoder()\ndata['species'] = le.fit_transform(data['species'])\n\n# Convert strings to numeric and handle any NaNs\ndata['sepal_length'] = pd.to_numeric(data['sepal_length'], errors='coerce')\ndata.dropna(inplace=True)\n\n# Split features and target\nX, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.18, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T16:22:48.747999Z","iopub.execute_input":"2024-11-10T16:22:48.749324Z","iopub.status.idle":"2024-11-10T16:22:48.784981Z","shell.execute_reply.started":"2024-11-10T16:22:48.749276Z","shell.execute_reply":"2024-11-10T16:22:48.783861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter grids\nrandom_forest_params = {\n    'n_estimators': [5, 10, 15, 20, 25],\n    'criterion': ['gini', 'entropy'],\n    'max_features': [2, 3, 4, 'auto', 'log2', 'sqrt', None],\n    'bootstrap': [False, True]\n}\ndecision_tree_params = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'min_samples_split': [2, 3, 4],\n    'max_features': [2, 3, 'auto', 'log2', 'sqrt', None],\n    'class_weight': ['balanced', None]\n}\nperceptron_params = {\n    'penalty': [None, 'l2', 'l1', 'elasticnet'],\n    'fit_intercept': [False, True],\n    'shuffle': [False, True],\n    'class_weight': ['balanced', None],\n    'alpha': [0.0001, 0.00025],\n    'max_iter': [30, 50, 90]\n}\nsvm_params = {\n    'shrinking': [False, True],\n    'degree': [3, 4],\n    'class_weight': ['balanced', None]\n}\nneural_net_params = {\n    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n    'hidden_layer_sizes': [(20, 15, 10), (30, 20, 15, 10), (16, 8, 4)],\n    'max_iter': [50, 80, 150],\n    'solver': ['adam', 'lbfgs'],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n    'shuffle': [True, False]\n}\nlog_reg_params = {\n    'class_weight': ['balanced', None],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n    'fit_intercept': [True, False]\n}\nknn_params = {\n    'n_neighbors': [2, 3, 5, 10],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size': [5, 10, 15, 20]\n}\nbagging_params = {\n    'n_estimators': [5, 12, 15, 20],\n    'bootstrap': [False, True]\n}\nada_boost_params = {\n    'n_estimators': [50, 75, 100],\n    'algorithm': ['SAMME', 'SAMME.R']\n}\ngradient_boosting_params = {\n    'n_estimators': [15, 25, 50]\n}\n\"\"\"\n# Build parameters of all classifiers \nYou can try this like difference \nrandom_forest_params = dict(n_estimators=[5, 10, 15, 20, 25], criterion=['gini', 'entropy'], \n                            max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None], bootstrap=[False, True]\n                            )\ndecision_tree_params = dict(criterion=['gini', 'entropy'], splitter=['best', 'random'], min_samples_split=[2, 3, 4],\n                            max_features=[2,3,'auto', 'log2', 'sqrt', None], class_weight=['balanced', None], presort=[False, True])\n\nperceptron_params = dict(penalty=[None, 'l2', 'l1', 'elasticnet'], fit_intercept=[False, True], shuffle=[False, True],\n                         class_weight=['balanced', None], alpha=[0.0001, 0.00025], max_iter=[30,50,90])\n\nsvm_params = dict(shrinking=[False, True], degree=[3,4], class_weight=['balanced', None])\n\nneural_net_params = dict(activation=['identity', 'logistic', 'tanh', 'relu'], hidden_layer_sizes = [(20,15,10),(30,20,15,10),(16,8,4)], \n                         max_iter=[50,80,150], solver=['adam','lbfgs'], learning_rate=['constant', 'invscaling', 'adaptive'], shuffle=[True, False])\n\nlog_reg_params = dict(class_weight=['balanced', None], solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'], fit_intercept=[True, False])\n\nknn_params = dict(n_neighbors=[2, 3, 5, 10], weights=['uniform', 'distance'],\n                  algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'], leaf_size=[5,10,15,20])\n\nbagging_params = dict(n_estimators=[5, 12, 15, 20], bootstrap=[False, True])\n\nada_boost_params = dict(n_estimators=[50, 75, 100], algorithm=['SAMME', 'SAMME.R'])\n\nguassiannb_params = dict()\n\ngradient_boosting_params = dict(n_estimators=[15, 25, 50])\n\nparams = [\n    random_forest_params, decision_tree_params, perceptron_params,\n    svm_params, neural_net_params, log_reg_params, knn_params,\n    bagging_params, ada_boost_params, guassiannb_params, gradient_boosting_params\n]\n\n# classifiers to test\nclassifiers = [\n    RandomForestClassifier(), DecisionTreeClassifier(), Perceptron(),\n    SVC(), MLPClassifier(), LogisticRegression(),\n    KNeighborsClassifier(), BaggingClassifier(), AdaBoostClassifier(),\n    GaussianNB(), GradientBoostingClassifier()\n]\n\nnames = [\n    'RandomForest', 'DecisionTree', 'Perceptron', 'SVM',\n    'NeuralNetwork', 'LogisticRegression',\n    'KNearestNeighbors', 'Bagging', 'AdaBoost', 'Naive-Bayes', 'GradientBoosting'\n]\n\nmodels = dict(zip(names, zip(classifiers, params)))\n\n\"\"\"\n\n# Models and their parameters\nmodels = {\n    'RandomForest': (RandomForestClassifier(), random_forest_params),\n    'DecisionTree': (DecisionTreeClassifier(), decision_tree_params),\n    'Perceptron': (Perceptron(), perceptron_params),\n    'SVM': (SVC(), svm_params),\n    'NeuralNetwork': (MLPClassifier(), neural_net_params),\n    'LogisticRegression': (LogisticRegression(), log_reg_params),\n    'KNearestNeighbors': (KNeighborsClassifier(), knn_params),\n    'Bagging': (BaggingClassifier(), bagging_params),\n    'AdaBoost': (AdaBoostClassifier(), ada_boost_params),\n    'Naive-Bayes': (GaussianNB(), {}),\n    'GradientBoosting': (GradientBoostingClassifier(), gradient_boosting_params)\n}\n\n# Parameter tuning with GridSearchCV\ndef parameter_tuning(models, X_train, X_test, y_train, y_test):\n    print(num_folds, 'fold cross-validation is used\\n')\n    accuracies = []\n    best_parameters = []\n    for name, (clf, clf_params) in models.items():\n        print(f'Computing GridSearch on {name}...')\n        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params, cv=num_folds)\n        grid_clf.fit(X_train, y_train)\n        best_parameters.append((name, grid_clf.best_params_))\n        predictions = grid_clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n        accuracies.append((name, accuracy, np.mean(cv_scores)))\n    return accuracies, best_parameters\n\n# Run parameter tuning and display results\nresults, best_parameters = parameter_tuning(models, X_train, X_test, y_train, y_test)\nprint('\\n============================================================')\nfor classifier, acc, cv_acc in results:\n    print(f'{classifier}: Accuracy with Best Parameters = {round(acc * 100, 4)}% || Mean Cross Validation Accuracy = {round(cv_acc * 100, 4)}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T16:23:02.345252Z","iopub.execute_input":"2024-11-10T16:23:02.346095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2.  Hyperparameter grids\nrandom_forest_params = dict(\n    n_estimators=[5, 10, 15, 20, 25],\n    criterion=['gini', 'entropy'],\n    max_features=[2, 3, 4, 'auto', 'log2', 'sqrt', None],\n    bootstrap=[False, True]\n)\ndecision_tree_params = dict(\n    criterion=['gini', 'entropy'],\n    splitter=['best', 'random'],\n    min_samples_split=[2, 3, 4],\n    max_features=[2, 3, 'auto', 'log2', 'sqrt', None],\n    class_weight=['balanced', None]\n)\nperceptron_params = dict(\n    penalty=[None, 'l2', 'l1', 'elasticnet'],\n    fit_intercept=[False, True],\n    shuffle=[False, True],\n    class_weight=['balanced', None],\n    alpha=[0.0001, 0.00025],\n    max_iter=[30, 50, 90]\n)\nsvm_params = dict(\n    shrinking=[False, True],\n    degree=[3, 4],\n    class_weight=['balanced', None]\n)\nneural_net_params = dict(\n    activation=['identity', 'logistic', 'tanh', 'relu'],\n    hidden_layer_sizes=[(20, 15, 10), (30, 20, 15, 10), (16, 8, 4)],\n    max_iter=[50, 80, 150],\n    solver=['adam', 'lbfgs'],\n    learning_rate=['constant', 'invscaling', 'adaptive'],\n    shuffle=[True, False]\n)\nlog_reg_params = dict(\n    class_weight=['balanced', None],\n    solver=['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n    fit_intercept=[True, False]\n)\nknn_params = dict(\n    n_neighbors=[2, 3, 5, 10],\n    weights=['uniform', 'distance'],\n    algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'],\n    leaf_size=[5, 10, 15, 20]\n)\nbagging_params = dict(\n    n_estimators=[5, 12, 15, 20],\n    bootstrap=[False, True]\n)\nada_boost_params = dict(\n    n_estimators=[50, 75, 100],\n    algorithm=['SAMME', 'SAMME.R']\n)\ngradient_boosting_params = dict(\n    n_estimators=[15, 25, 50]\n)\n\n# Models and their parameters\nmodels = {\n    'RandomForest': (RandomForestClassifier(), random_forest_params),\n    'DecisionTree': (DecisionTreeClassifier(), decision_tree_params),\n    'Perceptron': (Perceptron(), perceptron_params),\n    'SVM': (SVC(), svm_params),\n    'NeuralNetwork': (MLPClassifier(), neural_net_params),\n    'LogisticRegression': (LogisticRegression(), log_reg_params),\n    'KNearestNeighbors': (KNeighborsClassifier(), knn_params),\n    'Bagging': (BaggingClassifier(), bagging_params),\n    'AdaBoost': (AdaBoostClassifier(), ada_boost_params),\n    'Naive-Bayes': (GaussianNB(), {}),\n    'GradientBoosting': (GradientBoostingClassifier(), gradient_boosting_params)\n}\n\n# Parameter tuning with GridSearchCV\ndef parameter_tuning(models, X_train, X_test, y_train, y_test):\n    print(num_folds, 'fold cross-validation is used\\n')\n    accuracies = []\n    best_parameters = []\n    for name, (clf, clf_params) in models.items():\n        print(f'Computing GridSearch on {name}...')\n        grid_clf = GridSearchCV(estimator=clf, param_grid=clf_params, cv=num_folds)\n        grid_clf.fit(X_train, y_train)\n        best_parameters.append((name, grid_clf.best_params_))\n        predictions = grid_clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n        accuracies.append((name, accuracy, np.mean(cv_scores)))\n    return accuracies, best_parameters\n\n# Run parameter tuning and display results\nresults, best_parameters = parameter_tuning(models, X_train, X_test, y_train, y_test)\nprint('\\n============================================================')\nfor classifier_name, accuracy, cv_mean in results:\n    print(f\"{classifier_name} Accuracy on Test Set: {accuracy:.2f}, Mean CV Score: {cv_mean:.2f}\")\n\nprint('\\nBest Parameters for each model:')\nfor name, params in best_parameters:\n    print(f\"{name}: {params}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}